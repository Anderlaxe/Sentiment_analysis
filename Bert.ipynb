{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "import transformers\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "SEED = 1\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../../Data/train.csv')\n",
    "test = pd.read_csv('../../Data/test.csv')\n",
    "test_labels = pd.read_csv('../../Data/test_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "N_LABELS = len(LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "def preprocessComments(comment):\n",
    "    # Convert to lowercase, important for this bertmodel\n",
    "    comment = comment.lower()\n",
    "\n",
    "    # Remove leading and trailing spaces\n",
    "    comment = comment.strip()\n",
    "\n",
    "    # Remove consecutive spaces      \n",
    "    comment = re.sub(r' +', ' ', comment)\n",
    "\n",
    "    # Remove Newlines\n",
    "    comment = re.sub(r'\\n', ' ', comment)\n",
    "\n",
    "    return comment\n",
    "\n",
    "train.comment_text = train.comment_text.map(preprocessComments)\n",
    "train_y = train[LABELS].values\n",
    "test.comment_text = test.comment_text.map(preprocessComments)\n",
    "\n",
    "if train['comment_text'].isnull().values.any():\n",
    "  raise Exception(\"Missing data\")\n",
    "if test['comment_text'].isnull().values.any():\n",
    "  raise Exception(\"Missing data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_filtered = pd.merge(test, test_labels)\n",
    "test_filtered = test_filtered.drop(test_filtered.index[test_filtered['toxic'] == -1])\n",
    "comments_list = train['comment_text'].tolist()\n",
    "test_comments_list = test_filtered['comment_text'].tolist()\n",
    "print(test_filtered.shape)\n",
    "len(test_comments_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_name = \"bert-base-uncased\"\n",
    "\n",
    "bert_model = transformers.TFAutoModel.from_pretrained(bert_name)\n",
    "\n",
    "tokenizer = transformers.BertTokenizerFast.from_pretrained(\n",
    "    pretrained_model_name_or_path=bert_name, \n",
    "    config=transformers.BertConfig.from_pretrained(bert_name))\n",
    "\n",
    "TOKENS_MAX_LENGTH = 120\n",
    "\n",
    "def create_tokenizer(comments):\n",
    "  return tokenizer(\n",
    "    text=comments,\n",
    "    padding='longest', \n",
    "    truncation='longest_first',\n",
    "    max_length=TOKENS_MAX_LENGTH,\n",
    "    return_tensors='tf',\n",
    "    return_token_type_ids = False,\n",
    "    return_attention_mask = True)\n",
    "  \n",
    "train_tokenizer = create_tokenizer(comments_list)\n",
    "test_tokenizer = create_tokenizer(test_comments_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "att_mask = tf.keras.layers.Input(shape=(TOKENS_MAX_LENGTH,), name='attention_mask', dtype='int32') \n",
    "input_ids = tf.keras.layers.Input(shape=(TOKENS_MAX_LENGTH,), name='input_ids', dtype='int32')\n",
    "input = {'attention_mask': att_mask, 'input_ids': input_ids}\n",
    "x = bert_model.bert(input)\n",
    "\n",
    "x = tf.keras.layers.GlobalAveragePooling1D()(x[0])\n",
    "x = tf.keras.layers.Dense(N_LABELS, activation='sigmoid')(x)\n",
    "model = tf.keras.models.Model(input, x)\n",
    "\n",
    "model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4, decay=1e-5),\n",
    "    metrics=['acc']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    {'attention_mask': train_tokenizer['attention_mask'], 'input_ids': train_tokenizer['input_ids']},\n",
    "    train_y,\n",
    "    validation_split=0.2,\n",
    "    epochs=1,\n",
    "    batch_size=64,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model weights\n",
    "\n",
    "!mkdir -p saved_model\n",
    "model.save('saved_model/bert.h5') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(\n",
    "    {'attention_mask': test_tokenizer['attention_mask'], 'input_ids': test_tokenizer['input_ids']},\n",
    "    batch_size=64,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg = 0\n",
    "for i, label in enumerate(LABELS):\n",
    "    print(label, \":\")\n",
    "    pb = predictions[:, i] >= 0.5\n",
    "    score = f1_score(test_filtered[label], pb)\n",
    "    print(score)\n",
    "    avg += score\n",
    "\n",
    "avg /= N_LABELS\n",
    "print(\"Average f1-score:\", avg)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
